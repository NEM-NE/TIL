# 모두의 딥러닝 4 ~ 9강

## 4강 - Multi-Variable linear Regression

기존에는 한개의 변수만으로 처리를 했지만 만약 아래와 같은 상황이라면?

![스크린샷 2022-05-05 오후 11.27.20.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.27.20.png)

늘어난 변수에 맞게 가설이나 코스트 함수에 추가를 해준다.

![스크린샷 2022-05-05 오후 11.28.25.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.28.25.png)

![스크린샷 2022-05-05 오후 11.28.35.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.28.35.png)

그러나 만약에 x1, x2, x3, x4 ... 독립변수가 무수히 많으면 일일히 수식으로 표현하기 어려울 것이다.

그래서 Matrix(행렬을 이용을하는데) 아래 사진처럼 표현한다.

![스크린샷 2022-05-05 오후 11.30.53.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.30.53.png)

H(x) = xw는 행렬에 최적화된 식 

행렬을 사용하는 장점 중 하나는 인스턴스가 많을 경우 행렬에서 행을 늘리기만 하면 되기 때문에 한번 연산으로 모든 결과 값을 알 수 있다.

![스크린샷 2022-05-05 오후 11.33.35.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.33.35.png)

문제 → ?, ? 를 구하시오

![스크린샷 2022-05-05 오후 11.36.37.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_11.36.37.png)

⇒ 행렬의 곱의 특성을 이용하면 쉽게 구할 수 있는데 [5, 3] * [3, 1] → [5, 1]이 된다.

## 5강 - Logistic (regression) classification

분류 기법 중 하나 → Ex) 동그라미, 세모, 네모 분류하기

## What is Logistic Regression?

### Classification

→ 시험의 패스, 논패스 / 메일의 스팸메일 확인 / 인가받은 얼굴인지 아닌지 + 실제 사람의 얼굴인지

이진 분류에서는 참이 0 거짓이 1이며 학습 데이터를 통해서 0, 1의 결과를 준다.

### Logistic vs Linear

범주로 구분할 수 있다. → 범주형 데이터가 Y값

데이터들이 연속적이며 이어지는 데이터를 예측 해줌 → 수치형 데이터가 Y값

## How to solve?

### Hypothesis Representation

![스크린샷 2022-05-10 오후 2.45.11.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.45.11.png)

0과 1로만 표현을 한다. → 일반적인 linear function에서 나온 수치 값을 Logistic function을 거쳐 위 이미지와 같은 계단 모양의 선이 만들어지게 되고 Decision Boundary를 지정하여 0, 1를 출력하게 한다.

### Sigmoid/Logistic Function

![스크린샷 2022-05-10 오후 2.47.57.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.47.57.png)

→ Sigmoid 함수로 Z는 수치화 값이다. 즉, Z값이 크면 0으로 수렴하기 때문에 1이 되고  Z값이 작아지면 무한대로 수렴하기 때문에 0이 된다.

→ sigmoid 함수 그래프 자체가 위에서 나타나고자 했던 계단형태의 그래프 모양이다

### Decision Boundary

![스크린샷 2022-05-10 오후 2.50.15.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.50.15.png)

sigmoid 함수를 거쳐서 나온 Y값이 Decision Boundary를 통해 참이 될 것인지 거짓이 될 것인지를 결정한다.

![스크린샷 2022-05-10 오후 2.53.13.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.53.13.png)

### Cost Function

→ 초기 W 값을 최적의 값이 나올 수 있도록 도와주는 함수

![스크린샷 2022-05-10 오후 3.05.16.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.05.16.png)

### Optimizer(Gradient Descent)

코스트함수를 통해서 각각의 코스트를 알 수 있었는데 어떻게 최소화 할 수 잇는가?

Y축이 코스트이고 X축이 W값인 그래프에서 기울기를 최소화하면 된다.

![스크린샷 2022-05-10 오후 3.06.51.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.06.51.png)

![스크린샷 2022-05-10 오후 3.08.17.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.08.17.png)

위 수식을 반복하면서 최적화를 한다. → W = W - (learningRate  * W에 대한 Gradient값)

## Summary

![스크린샷 2022-05-10 오후 3.21.03.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.21.03.png)

Classification의 과정은 뉴럴 네트워크의 과정과 유사하다.

x값이 들어가면 linear한 값이나오고 Activation 함수를 통해 우리가 원하는 0, 1의 값이 나온다.

## 6강 - Softmax

기존 Binary Classification에서는 2가지에 범주에 한해서 분류가 가능했다.

그러나 여러개가 있을 때에도 Binary Classfication을 사용하면 분류가 가능함

![스크린샷 2022-05-10 오후 3.49.49.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.49.49.png)

즉, 3개의 W → linear → sigmoid → {1. 0}인 수식을 만들면 되는데

![스크린샷 2022-05-10 오후 3.50.35.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.50.35.png)

이는 다시 행렬을 통해서 간단하게 linear 수식을 나타낼 수 있다.

그러나 이 결과 값은 우리가 원하는 0 ~ 1사이의 값이 아님 → sigmoid 과정이 필요함

이것을 해주는 것이 softmax(softmax는 activation 함수이다)

![스크린샷 2022-05-10 오후 3.54.56.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.54.56.png)

→  여기서 결과값만 알려주는 원핫인코딩을 사용

![스크린샷 2022-05-10 오후 3.55.31.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.55.31.png)

이제 모델을 만들었으면 모델을 최적화하기 위해 cost function이 필요함

cost function은 기존 mse 방식과는 다르게 cross-entropy 방식을 사용한다.

![스크린샷 2022-05-10 오후 3.57.34.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-10_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.57.34.png)

이후 Optimizer는 Gradient Descent 알고리즘을 통해 기울기가 최소인 지점을 찾는다.

## 7강 - Application Tip

## 8강 - DeepLearning

생각하는 기계를 만들기 위해 우리의 뇌를 기반으로 만들어보기 시작함

![스크린샷 2022-05-11 오후 12.37.06.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.37.06.png)

![스크린샷 2022-05-11 오후 12.37.18.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.37.18.png)

뉴런의 구조를 코드 상으로 만들어 보면 다음과 같았다. 즉, 인풋들의 합을 만들고 편향값을 더해주고  activation function을 거쳐 아웃풋으로 출력한다.

![스크린샷 2022-05-11 오후 12.44.58.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.44.58.png)

그러나 OR, AND 문제는 풀 수 있었지만 XOR 문제는 선을 통해 풀지 못했음

→ 여러개의 선을 통해서 해결할 수 있음

→ 그러나 여러개의 선에 따른 w, b값을 학습시키지 못해서 과거에는 구현을 못했음

이런 문제는 BackPropagation 알고리즘을 통해서 여러개의 w, b 을 수정할 수 있었음

![스크린샷 2022-05-11 오후 12.48.23.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.48.23.png)

그러나 히든 레이어가 많아질수록 에러가 제대로 전달되지 않아 성능이 떨어지기 시작함

→ 초기 가중치 편향 값을 잘 지정해주면 해결할 수 있음

## 9강 - Neural Nets(NN) for XOR

XOR 문제는 여러개의 로지스틱으로 해결할 수 있었음

![스크린샷 2022-05-11 오후 8.30.57.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_8.30.57.png)

위 과정이 Forward propagation이다.

![스크린샷 2022-05-11 오후 8.34.55.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_8.34.55.png)

![스크린샷 2022-05-11 오후 8.37.22.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_8.37.22.png)

이 전 그림처럼 2가지를 병렬로 나타내지 말고 한번에 나타낼 수 있는데 이를 행렬의 곱으로 표현이 가능하다.

![스크린샷 2022-05-11 오후 8.38.50.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_8.38.50.png)

Nerual Net은 여러개로 묶인 식들을 행렬을 통해 하나로 만들고 이것을 계속 이어서 만든 것

W1, W2, B1, B2를 어떻게 학습할 수 있을까? → Gradient Descent 함수를 이용

그러나 여러개의 레이어로 중첩된 NN은 학습하기에는 어려웠음 → BackPropagation 알고리즘 이용

![스크린샷 2022-05-11 오후 8.50.53.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_8.50.53.png)

chain rule → 함성함수 미분과 비슷

foward → 실제 값을 대입에서 계산, 결과값을 얻을 수 있음

back →  합성함수 미분 & 편미분을 통해 w, x, b가 각각 결과값에 얼마나 영향을 미치는 지 알 수 잇다.

![스크린샷 2022-05-11 오후 8.55.31.png](%E1%84%86%E1%85%A9%E1%84%83%E1%85%AE%E1%84%8B%E1%85%B4%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%204%20~%209%E1%84%80%E1%85%A1%E1%86%BC%20446a318f8ca7462ca1ca2ef7f23037c4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-05-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_8.55.31.png)

이런 복잡한 과정을 텐서플로우에서는 알아서 해주고 있다.